{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkUXvc7JZqOV"
      },
      "outputs": [],
      "source": [
        "# Needed Libraries\n",
        "!pip install tensorflow\n",
        "!pip install tflearn\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the tflearn graph from earlier training\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()"
      ],
      "metadata": {
        "id": "qxhdJJ9yaFRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "YkFGmQ49aKAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "StCnaXvwaM3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load any of the benchmark dataset\n",
        "data=json.load(open('/content/drive/MyDrive/data/HWU65.json')) # Put the path to the dataset where you saved it"
      ],
      "metadata": {
        "id": "W3__Ck94aNz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(data, columns=['text', 'label'])\n",
        "test_df = pd.DataFrame(data['test'], columns=['text', 'label'])\n",
        "val_df = pd.DataFrame(data['val'], columns=['text', 'label']) #Excempted for Banking77 Dataset\n",
        "print(\"Len(train):\", len(train_df))\n",
        "print(\"Len(test):\", len(test_df))\n",
        "print(\"Len(val):\", len(val_df))"
      ],
      "metadata": {
        "id": "bfzG0D_baWxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show label unique\n",
        "print(np.unique(train_df[\"label\"]))\n",
        "total_label = int(len(np.unique(train_df[\"label\"])))\n",
        "print(\"\\n ====>   Total: \", total_label)"
      ],
      "metadata": {
        "id": "QDrEND5WbZCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check NaN data\n",
        "train_df.isna().sum()"
      ],
      "metadata": {
        "id": "1XWOEiHjboXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check balance data\n",
        "train_df['label'].value_counts()[:50]"
      ],
      "metadata": {
        "id": "rM8Sx4JkbtWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick 30 samples each intents or labels. We only experimented on the stated sample size per label due to memeroy constraint and to make comparisons with other approach on same proportion\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, test_x, train_y, test_y = train_test_split(train_df['text'], train_df['label'], random_state=20, train_size=0.30, stratify=train_df['label'])"
      ],
      "metadata": {
        "id": "LSMT5gUAbzWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your new dataset of 30 samples for each intents or labels\n",
        "train_df= pd.DataFrame({'text':train_x, 'label':train_y})"
      ],
      "metadata": {
        "id": "Uqfq4e4zcaMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "classes = []\n",
        "words  = []\n",
        "ignore_words = ['?']\n",
        "\n",
        "#loop through each sentence in  your 30 samples for each labels collect them into a document container\n",
        "for index, row in train_df.iterrows():\n",
        "  for i in range(len(row['text'])):\n",
        "    #tokenize each word in the pattern\n",
        "    w = nltk.word_tokenize(row['text'])\n",
        "    #add to our words list\n",
        "    words.extend(w)\n",
        "    #add to document in our corpus\n",
        "    documents.append((w, row['label']))\n",
        "    #add to our classes list\n",
        "    if row['label'] not in classes:\n",
        "      classes.append(row['label'])\n",
        "\n",
        "#stem and lower each word and remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "#remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print(len(documents), 'documents')\n",
        "print(len(classes), 'classes', classes)\n",
        "print(len(words), 'unique stemmed words', words)"
      ],
      "metadata": {
        "id": "DX7kxIkycs2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating our training data\n",
        "training = []\n",
        "output = []\n",
        "#create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "#training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "  #initialize our bag of words\n",
        "  bag = []\n",
        "  #list of tokenized words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "  # stem each word\n",
        "  pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "  # create our bag od words array\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "metadata": {
        "id": "QubuIx5HdMyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "id": "ohwMuzDjfGAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "JntSkV7FfOEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reset underlying graph data\n",
        "tf.reset_default_graph()\n",
        "# Build neural network\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.embedding(net, input_dim=len(words), output_dim=10,  trainable=True, name=\"EmbeddingLayer\")\n",
        "# ELM Layer\n",
        "net = tflearn.fully_connected(net, 128)\n",
        "net = tflearn.activations.tanh(net)\n",
        "\n",
        "# create attention layer\n",
        "attention_probs = tflearn.fully_connected(net, 128, activation='softmax')\n",
        "attention_mul = tf.multiply(net, attention_probs)\n",
        "\n",
        "# add fully connected layer and output layer\n",
        "net = tflearn.fully_connected(attention_mul, len(train_y[0]), activation='softmax')\n",
        "#net = tflearn.regression(net)\n",
        "net = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy', metric='default', learning_rate=0.001)\n",
        "\n",
        "# Define model and setup tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "# Start training (apply gradient descent algorithm)\n",
        "hist=model.fit(train_x, train_y, n_epoch=200, batch_size=128, show_metric=True, validation_set=(test_x, test_y))\n",
        "\n",
        "model.save('model.tflearn')"
      ],
      "metadata": {
        "id": "DLivAAE7e4lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "training_time = end_time - start_time\n",
        "print(\"Training time: \", training_time, \"seconds\")"
      ],
      "metadata": {
        "id": "5dXSN0hGfPIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_x, test_y)"
      ],
      "metadata": {
        "id": "HivWOuVffVJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing the test for F1, precision and recall scores computation\n",
        "import sklearn.metrics\n",
        "Y_pred = model.predict(test_x)\n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1)\n",
        "\n",
        "import numpy as np\n",
        "test_y = np.argmax(test_y, axis=1)"
      ],
      "metadata": {
        "id": "DEp0OG1hf88J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate precision, recall and F1 score\n",
        "import sklearn.metrics\n",
        "sklearn.metrics.precision_recall_fscore_support(Y_pred_classes, test_y,  beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')"
      ],
      "metadata": {
        "id": "eBA0-I80gR3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}