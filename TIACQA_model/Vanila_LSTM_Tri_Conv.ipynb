{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install tflearn"
      ],
      "metadata": {
        "id": "NFvzmeJiGwCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# things we need for NLP\n",
        "import nltk\n",
        "#from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "#stemmer = LancasterStemmer()\n",
        "import pandas as pd\n",
        "\n",
        "# things we need for Tensorflow\n",
        "import numpy as np\n",
        "#import tflearn\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "id": "cM-FD8JRGGx0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()"
      ],
      "metadata": {
        "id": "_5jiem2IGMAw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "u2d3rM_UGMwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "d-9qTsVWY7M2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "import json\n",
        "with open('intents_eng_update.json', 'r') as json_data:\n",
        "  intents = json.load(json_data)\n",
        "\n",
        "# Load yoruba dataset\n",
        "with open('intent_yoruba_update.json', 'r') as json_data:\n",
        "  intents_yo = json.load(json_data)\n",
        "\n",
        "# Load hindi dataset\n",
        "with open('intent_hindi_update.json', 'r') as json_data:\n",
        "  intents_hi = json.load(json_data)"
      ],
      "metadata": {
        "id": "m6cw03IZHt-d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#With our intents JSON file loaded, we can now begin to organize our documents, words and classification classes.\n",
        "documents = []\n",
        "classes = []\n",
        "words  = []\n",
        "ignore_words = ['?']\n",
        "\n",
        "#loop through each sentence in our intent file pattern in english dataset\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #tokenize each word in the pattern\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    #add to our words list\n",
        "    words.extend(w)\n",
        "    #add to document in our corpus\n",
        "    documents.append((w, intent['tag']))\n",
        "    #add to our classes list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "\n",
        "#loop through each sentence in our intent file pattern yoruba dataset\n",
        "for intent in intents_yo['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #tokenize each word in the pattern\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    #add to our words list\n",
        "    words.extend(w)\n",
        "    #add to document in our corpus\n",
        "    documents.append((w, intent['tag']))\n",
        "    #add to our classes list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "\n",
        "#loop through each sentence in our intent file pattern hindi dataset\n",
        "for intent in intents_hi['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #tokenize each word in the pattern\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    #add to our words list\n",
        "    words.extend(w)\n",
        "    #add to document in our corpus\n",
        "    documents.append((w, intent['tag']))\n",
        "    #add to our classes list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "#stem and lower each word and remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "#remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print(len(documents), 'documents')\n",
        "print(len(classes), 'classes', classes)\n",
        "print(len(words), 'unique stemmed words', words)"
      ],
      "metadata": {
        "id": "n737LWX6HvGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating our training data\n",
        "training = []\n",
        "output = []\n",
        "#create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "#training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "  #initialize our bag of words\n",
        "  bag = []\n",
        "  #list of tokenized words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "  # stem each word\n",
        "  pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "  # create our bag od words array\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "metadata": {
        "id": "vlQYTMU0U-kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.2, random_state=10)"
      ],
      "metadata": {
        "id": "-1tXlZyDWN1Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)\n",
        "test_x=np.array(test_x)\n",
        "test_y = np.array(test_y)"
      ],
      "metadata": {
        "id": "EHlfQePrWel_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, LSTM\n",
        "from nltk import metrics\n",
        "vocab_size = len(words)\n",
        "model = keras.Sequential()\n",
        "e = Embedding(vocab_size, 200, input_length=len(train_x[0]), trainable=False)\n",
        "model.add(e)\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "# Summarize the model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "Km7frgU4Vw-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history = model.fit(train_x, train_y, epochs=50, batch_size=16)"
      ],
      "metadata": {
        "id": "Vv53zApIWoKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_x, test_y)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "NUHfVOCqWtUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}