{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2DOZ5WYXkf",
        "outputId": "7ab6c902-0f6f-4460-cdf9-4b1162a34bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (8.4.0)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127283 sha256=67d001184a6e77fdcf5689c455d76d997f59c51a872f33289f9f06d018b8a2b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/fb/7b/e06204a0ceefa45443930b9a250cb5ebe31def0e4e8245a465\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "# Needed Libraries\n",
        "!pip install tensorflow\n",
        "!pip install tflearn\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XOGjq8kJYho7"
      },
      "outputs": [],
      "source": [
        "# Reset the tflearn graph from earlier training\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8URQ30lYsRI",
        "outputId": "4e149f77-e741-4823-8910-7299d80e41c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pOz2jvXhY4L5"
      },
      "outputs": [],
      "source": [
        "stemmer = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2sdNf8EKn9H",
        "outputId": "5655ef1f-9202-4b75-c5a6-831bd87c7131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ELM\n"
          ]
        }
      ],
      "source": [
        "# Set the directory path for loading the datasets\n",
        "%cd /content/drive/MyDrive/ELM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ELwtUebxZKPG"
      },
      "outputs": [],
      "source": [
        "# Load the Trilingual Dataset\n",
        "import json\n",
        "with open('intents_eng_update.json', 'r') as json_data:\n",
        "  intents = json.load(json_data)\n",
        "\n",
        "# Load yoruba dataset\n",
        "with open('intent_yoruba_update.json', 'r') as json_data:\n",
        "  intents_yo = json.load(json_data)\n",
        "\n",
        "# Load hindi dataset\n",
        "with open('intent_hindi_update.json', 'r') as json_data:\n",
        "  intents_hi = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEAI7tlyhPaO"
      },
      "outputs": [],
      "source": [
        "intents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4iIjZF5Zgod",
        "outputId": "bcf691ca-ed41-45e2-f660-01fefb4bfc97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143 documents\n",
            "44 classes ['O dabọ', 'alavida', 'apanilẹrin', 'awọn nkan', 'bhugataan', 'delivery', 'funny', 'goodbye', 'greeting', 'idupẹ', 'ifijiṣẹ', 'igbaradi akoko', 'ikini', 'iru ile', 'items', 'majedaar', 'mittee ke prakaar', 'nutrition', 'orisi', 'ounje', 'owo sisan', 'payments', 'poshan', 'prakaar', 'preparation time', 'saamaan', 'shubhakaamana', 'shukriya', 'soil type', 'taiyaaree ka samay', 'thanks', 'types', 'vitaran', 'अलविदा', 'तैयारी का समय', 'पोषण', 'प्रकार', 'भुगतान', 'मजेदार', 'मिट्टी के प्रकार', 'वितरण', 'शुक्रिया', 'शुभकामना', 'सामान']\n",
            "335 unique stemmed words ['!', \"'s\", 'a', 'aadarsh', 'aap', 'aapak', 'aapako', 'abhe', 'abhivaadan', 'accept', 'achchha', 'achchhe', 'akoko', 'alavida', 'anfani', 'anyon', 'apanilẹrin', 'ara', 'are', 'awada', 'awọn', 'baad', 'baay', 'bahut-bahut', 'banaan', 'bataen', 'batao', 'bawo', 'bechat', 'benefit', 'best', 'bhugataan', 'bodi', 'bye', 'can', 'card', 'cash', 'chaay', 'chutakula', 'coffe', 'credit', 'dabọ', 'dada', 'dara', 'day', 'deliveri', 'dhanyavaad', 'dileevare', 'din', 'do', 'doe', 'drink', 'dupe', 'e', 'ek', 'for', 'frame', 'fun', 'funni', 'gamal', 'gba', 'gbigb', 'get', 'good', 'goodby', 'grade', 'gred', 'haal', 'hai', 'hain', 'hamaar', 'have', 'hello', 'help', 'hey', 'hi', 'ho', 'hoge', 'hoon', 'how', 'i', 'ideal', 'ifijiṣẹ', 'igba', 'ile', 'ipel', 'iranlọwọ', 'iru', 'is', 'item', 'iyẹn', 'jaanat', 'jok', 'joke', 'ju', 'julọ', 'jẹ', 'ka', 'kaadi', 'kaard', 'kaaro', 'kaasan', 'kab', 'kais', 'kan', 'kar', 'karan', 'karat', 'kaun', 'ke', 'kee', 'keval', 'kind', 'kini', 'kirẹditi', 'kis', 'kitana', 'know', 'ko', 'koee', 'kofi', 'kophe', 'kredit', 'kuchh', 'kya', 'kọfi', 'laabh', 'lagata', 'laip', 'later', 'lati', 'le', 'lete', 'lie', 'long', 'lot', 'lọpọlọpọ', 'ma', 'madadagaar', 'main', 'make', 'masita', 'mastairchard', 'mastercard', 'mazedaar', 'me', 'mein', 'melo', 'mere', 'mi', 'milat', 'mimu', 'mishran', 'mitte', 'mix', 'mo', 'mujh', 'my', 'mọ', 'n', 'na', 'nakad', 'namast', 'ngba', 'ni', 'nibẹ', 'nigbawo', 'nikan', 'nkan', 'nkankan', 'nutrit', 'o', 'odigba', 'of', 'ojo', 'onj', 'onli', 'our', 'owo', 'paa', 'pahunchaate', 'paudh', 'pay', 'paypal', 'pepail', 'plant', 'poshan', 'pot', 'prakaar', 'prepar', 'pẹlu', 'pẹlẹ', 'ri', 'saath', 'sabas', 'sakata', 'samay', 'sambandhe', 'sanwo', 'se', 'see', 'seema', 'sell', 'shareer', 'ship', 'shipe', 'so', 'soil', 'someth', 'sveekaar', 'sọ', 'ta', 'taiyaar', 'take', 'tarah', 'tell', 'thank', 'that', 'the', 'there', 'ti', 'tii', 'time', 'to', 'tum', 'type', 'vaale', 'vahaan', 'vastuen', 'wa', 'what', 'when', 'which', 'with', 'wo', 'yah', 'you', 'अच्छा', 'अच्छी', 'अभिवादन', 'अभी', 'अरे', 'अलविदा', 'आदर्श', 'आप', 'आपके', 'आपको', 'एक', 'कब', 'कर', 'करते', 'करने', 'का', 'कार्ड', 'कितना', 'किस', 'की', 'कुछ', 'के', 'केवल', 'कैसे', 'कॉफी', 'को', 'कोई', 'कौन', 'क्या', 'क्रेडिट', 'गमले', 'ग्रेड', 'चाय', 'चुटकुला', 'जानते', 'जोक', 'डिलीवरी', 'ड्रिंक', 'तरह', 'तुम', 'तैयार', 'दिन', 'धन्यवाद', 'नकद', 'नमस्ते', 'पहुंचाती', 'पास', 'पेपैल', 'पोषण', 'पौधे', 'प्रकार', 'बताएं', 'बताओ', 'बनाने', 'बहुत-बहुत', 'बाद', 'बाय', 'बेचते', 'भुगतान', 'मज़ेदार', 'मददगार', 'मिट्टी', 'मिलते', 'मिश्रण', 'मुझे', 'में', 'मेरी', 'मैं', 'यह', 'लगता', 'लाभ', 'लिए', 'लेते', 'वस्तुएं', 'वहां', 'वाली', 'शरीर', 'शिपिंग', 'संबंधी', 'सकता', 'सबसे', 'समय', 'साथ', 'सी', 'सीमा', 'स्वीकार', 'हमारे', 'हाल', 'हूँ', 'है', 'हैं', 'हो', 'होगी', 'ṣe', 'ṣeto', 'ṣeun', 'ẹnikẹni', 'ọgbin']\n"
          ]
        }
      ],
      "source": [
        "# After loading the JSON file, begin collect of the three langauge into documents, words and intent class.\n",
        "\n",
        "documents = []\n",
        "classes = []\n",
        "words  = []\n",
        "ignore_words = ['?']\n",
        "\n",
        "#loop through each sentence in our intent file pattern in english dataset\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #tokenize each word in the pattern\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    #add to our words list\n",
        "    words.extend(w)\n",
        "    #add to document in our corpus\n",
        "    documents.append((w, intent['tag']))\n",
        "    #add to our classes list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "\n",
        "#loop through each sentence in our intent file pattern yoruba dataset\n",
        "for intent in intents_yo['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #tokenize each word in the pattern\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    #add to our words list\n",
        "    words.extend(w)\n",
        "    #add to document in our corpus\n",
        "    documents.append((w, intent['tag']))\n",
        "    #add to our classes list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "\n",
        "#loop through each sentence in our intent file pattern hindi dataset\n",
        "for intent in intents_hi['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #tokenize each word in the pattern\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    #add to our words list\n",
        "    words.extend(w)\n",
        "    #add to document in our corpus\n",
        "    documents.append((w, intent['tag']))\n",
        "    #add to our classes list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "#stem and lower each word and remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "#remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print(len(documents), 'documents')\n",
        "print(len(classes), 'classes', classes)\n",
        "print(len(words), 'unique stemmed words', words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jrt_4b8aIb3",
        "outputId": "a2d8a39e-9c82-4e20-eac0-757fa3a2bec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9344e34e6ec6>:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.array(training)\n"
          ]
        }
      ],
      "source": [
        "#creating our training data\n",
        "training = []\n",
        "output = []\n",
        "#create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "#training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "  #initialize our bag of words\n",
        "  bag = []\n",
        "  #list of tokenized words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "  # stem each word\n",
        "  pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "  # create our bag od words array\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Czl3pDzFaME6"
      },
      "outputs": [],
      "source": [
        "# Split into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.20, random_state=10, stratify=train_y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x"
      ],
      "metadata": {
        "id": "K-PSO-6fvKrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2Rcws93daVzW"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jGNv9895XQgC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Your model training code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt-h1aWIHqFr",
        "outputId": "1789b83e-d7ad-4176-ecf9-db55eb83ec74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 29949  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 8.236s\n",
            "| Adam | epoch: 050 | loss: 0.00002 - acc: 1.0000 -- iter: 38272/38324\n",
            "Training Step: 29950  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 9.576s\n",
            "| Adam | epoch: 050 | loss: 0.00002 - acc: 1.0000 | val_loss: 0.00002 - val_acc: 1.0000 -- iter: 38324/38324\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "# reset underlying graph data\n",
        "tf.reset_default_graph()\n",
        "# Build neural network\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.embedding(net, input_dim=len(words), output_dim=10,  trainable=True, name=\"EmbeddingLayer\")\n",
        "# ELM Layer\n",
        "net = tflearn.fully_connected(net, 128)\n",
        "net = tflearn.activations.tanh(net)\n",
        "\n",
        "# create attention layer\n",
        "attention_probs = tflearn.fully_connected(net, 1, activation='softmax')\n",
        "attention_mul = tf.multiply(net, attention_probs)\n",
        "\n",
        "# add fully connected layer and output layer\n",
        "net = tflearn.fully_connected(attention_mul, len(train_y[0]), activation='softmax')\n",
        "#net = tflearn.regression(net)\n",
        "net = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy', metric='default', learning_rate=0.001)\n",
        "\n",
        "# Define model and setup tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "# Start training (apply gradient descent algorithm)\n",
        "hist=model.fit(train_x, train_y, n_epoch=50, batch_size=64, show_metric=True, validation_set=(test_x, test_y))\n",
        "\n",
        "model.save('model.tflearn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgvIgOL6XUWa",
        "outputId": "1e762858-70e4-498c-d740-58e099553b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time:  626.1890141963959 seconds\n"
          ]
        }
      ],
      "source": [
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "training_time = end_time - start_time\n",
        "print(\"Training time: \", training_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1A0_jQSIYlb"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_x, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFaX5YTqIZOE"
      },
      "outputs": [],
      "source": [
        "predicted_x = model.predict(test_x)\n",
        "#y_hatclass = model.predict_classes(test_x)\n",
        "predicted_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYFaEAORIl_v"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "#METHOD 1\n",
        "#accuracy = model.evaluate(test_x, test_y)\n",
        "\n",
        "#METHOD 2\n",
        "predictions = model.predict(test_x)\n",
        "accuracy = 0\n",
        "for prediction, actual in zip(predictions, test_y):\n",
        "    predicted_class = numpy.argmax(prediction)\n",
        "    actual_class = numpy.argmax(actual)\n",
        "    if(predicted_class == actual_class):\n",
        "        accuracy+=1\n",
        "accuracy = accuracy / len(test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEB8vy1WM4ru"
      },
      "outputs": [],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIfxj_pjIzSQ"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics\n",
        "Y_pred = model.predict(test_x)\n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1)\n",
        "\n",
        "import numpy as np\n",
        "test_y = np.argmax(test_y, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHp3kJOxI5yY"
      },
      "outputs": [],
      "source": [
        "#calculate precision, recall and F1 score\n",
        "import sklearn.metrics\n",
        "sklearn.metrics.precision_recall_fscore_support(Y_pred_classes, test_y,  beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iM-TUACI9wj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "confusion_mtx = sklearn.metrics.confusion_matrix(test_y, Y_pred_classes)\n",
        "f,ax = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Blues\",linecolor=\"black\", fmt= '.1f',ax=ax)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dAjtNhDkJjbe"
      },
      "outputs": [],
      "source": [
        "# Prepare user input for onward dialogue\n",
        "def clean_up_sentence(sentence):\n",
        "  #tokenize the pattern\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  # stem each word\n",
        "  sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "  return sentence_words\n",
        "\n",
        "# return bag of words array : 0 or 1 for each word in the bag that exit in the sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "  # tokenize the pattern\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  # bag of words\n",
        "  bag = [0]*len(words)\n",
        "  for s in sentence_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w == s:\n",
        "        bag[i] = 1\n",
        "        if show_details:\n",
        "          print(\"found in bag: %s\" %w)\n",
        "  return(np.array(bag))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0UkdbdHLetw"
      },
      "outputs": [],
      "source": [
        "# Dialouge section in which response to user inputs are retrieved based on a probabilty cut off\n",
        "\n",
        "agent_name = 'Coffee chat'\n",
        "ERROR_THRESHOLD = 0.75\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRGmA-46MaPD",
        "outputId": "5a00a4c2-11e7-4709-ec65-811bdf0be0ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "classify('what  payment method do you accept')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dialogue module\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "  results = classify(sentence)\n",
        "  # if we have a classification then find the matching intent tag\n",
        "  if results:\n",
        "    # loop as long as there are matches to process\n",
        "    while results:\n",
        "\n",
        "      for i in intents['intents']:\n",
        "        # find a tag matching the first result\n",
        "        if i['tag'] == results[0][0]:\n",
        "          # a random response from the intent\n",
        "          return print(f\"{agent_name}: {random.choice(i['responses'])}\")\n",
        "\n",
        "      for a in intents_yo['intents']:\n",
        "        # find a tag matching the first result\n",
        "        if a['tag'] == results[0][0]:\n",
        "          # a random response from the intent\n",
        "          return print(f\"{agent_name}: {random.choice(a['responses'])}\")\n",
        "      for b in intents_hi['intents']:\n",
        "        # find a tag matching the first result\n",
        "        if b['tag'] == results[0][0]:\n",
        "          # a random response from the intent\n",
        "          return print(f\"{agent_name}: {random.choice(b['responses'])}\")\n",
        "\n",
        "    results.pop(0)\n",
        "  else:\n",
        "      print(f'{agent_name} : I do not understand kindly rephrase your question...')\n",
        "\n",
        "# Define quiting command in either of the languages\n",
        "eng=\"quit\"\n",
        "yor=\"jáwọ́\"\n",
        "hind=\"छोड़ना\"\n",
        "hineng=\"chhodana\"\n",
        "\n",
        "\n",
        "print(\"Coffee Chat: Hi! How can I assist you today? \\nif you wich to end the conversation type Quit or jáwọ́ or छोड़ना or chhodana\")\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() == eng:\n",
        "        print(f\"{agent_name}: Goodbye!\")\n",
        "        break\n",
        "    if user_input.lower() == yor:\n",
        "        print(f\"{agent_name}: O dabọ o ṣeun fun àbẹwò\")\n",
        "        break\n",
        "    if user_input.lower() == hineng:\n",
        "        print(f\"{agent_name}: alavida aane ke lie dhanyavaad\")\n",
        "        break\n",
        "    if user_input.lower() == hind:\n",
        "        print(f\"{agent_name}: अलविदा आने के लिए धन्यवाद\")\n",
        "        break\n",
        "    response(user_input, show_details=True)\n"
      ],
      "metadata": {
        "id": "F8jKyY4V5Xxb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}